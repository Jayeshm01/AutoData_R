txtparts = str_replace(txtparts,"^\n","")
txtparts = txtparts[txtparts!=""]
year = str_extract(txtparts,"[[:digit:]]{2}  Average Air Temperature")
year = str_extract(year,"[[:digit:]]{2}")
year = ifelse(year < 20, str_c(20,year), str_c(19,year))
year = as.numeric(year)
station = str_extract(txtparts, "Station : .+?\n")
station     = str_replace_all(station, "(Station : )|(\n)", "")
station     = str_split(station,", ")
id          = sapply(station, '[', 1)
name        = sapply(station, '[', 2)
temperatures = str_extract(txtparts, "day[\\s\\S]*")
tempData = data.frame(avgtemp=NA, day=NA, month=NA, year=NA, id="", name="")
day      = rep(1:31, 12)
month    = rep( c(10:12,1:9), each=31 )
doTemp = function(temperatures, year, name, id){
tf = tempfile()
writeLines(temperatures, tf)
temptable = read.fwf(tf, width=c(3,7,6,6,6,6,6,6,6,6,6,6,6),
stringsAsFactors=F)
temptable = temptable[3:33, -1]
temptable = suppressWarnings(as.numeric(unlist(temptable)))
temptable = data.frame( avgtemp=temptable, day=day, month=month,
year=year, name=name, id=id )
tempData <= rbind(tempData, temptable)
}
mapply(doTemp, temperatures, year, name, id)
tempData = tempData[!is.na(tempData$avgtemp),]
return(tempData)
}
parseTemps = function(filenames){
tmp = lapply(filenames, parseTemp)
tempData = NULL
for(i in seq_along(tmp)) tempData = rbind(tempData,tmp[[i]])
return(tempData)
}
tempData = parseTemps( str_c("Data/",filesavg) )
download.file("ftp://ftp.wcc.nrcs.usda.gov/states/ca/jchen/CA_sites.dat",
"Data/CA_sites.dat")
stationData = read.csv("data/CA_sites.dat", header=F, sep="|")[,-c(1,2,7:9)]
names(stationData) = c("name","lat","lon","alt")
stationData$lon = stationData$lon * -1
stationData[,c("lat","lon")] = stationData[,c("lat","lon")]/100
stationData$alt = stationData$alt / 3.2808399
stationData = stationData[order(stationData$lat),]
center  = c(mean(range(stationData$lat)),mean(range(stationData$lon)))
map1 = GetMap(
destfile = "map1.png",
zoom=7,
size=c(640,500),
GRAYSCALE=T,
center=center,
maptype="hybrid",
NEWMAP=FALSE)
map2 = GetMap.OSM(
latR=c(37.5,42),
lonR=c(-125,-115),
scale=5000000,
destfile = "map2.png",
GRAYSCALE=TRUE,
NEWMAP=FALSE)
tmp = readPNG("map2.png", native = FALSE)
tmp = RGB2GRAY(tmp)
writePNG(tmp, "map2.png")
png("stationmap1.png",
width=dim(readPNG("map1.png"))[2],
height=dim(readPNG("map1.png"))[1])
PlotOnStaticMap(map1,
lat = stationData$lat,
lon = stationData$lon,
cex=2, pch=19,
col=rgb(1,1,1,0.8), add=FALSE)
dev.off()
png("stationmap2.png",
width=dim(readPNG("map2.png"))[2],
height=dim(readPNG("map2.png"))[1])
PlotOnStaticMap(map2,
lat = stationData$lat,
lon = stationData$lon,
cex=2, pch=19,
col=rgb(0,0,0,0.5), add=FALSE)
dev.off()
monthlyTemp = aggregate(tempData$avgtemp, by=list(name=tempData$name,month=tempData$month), mean)
stationNames = c(  "ADIN MTN", "INDEPENDENCE CAMP", "SQUAW VALLEY G.C.",
"SPRATT CREEK", "LEAVITT MEADOWS","POISON FLAT")
stationAlt   = stationData[match(stationNames, stationData$name),]$alt
stationLat   = stationData[match(stationNames, stationData$name),]$lat
stationLon   = stationData[match(stationNames, stationData$name),]$lon
plotTemps = function(i){
iffer = monthlyTemp$name==stationNames[i]
plot( monthlyTemp[iffer, c("month","x")],
type="b",
main=str_c(stationNames[i],
" (",round(stationAlt[i]),
"m)",
"\n Lat.= ", stationLat[i],
" Lon.= ",   stationLon[i]),
ylim=c(-15,25),
ylab="average temperature" )
abline(h=0,lty=2)
iffer2 = tempData$name==stationNames[i]
points( tempData$month[iffer2] + tempData$day[iffer2] *0.032,
jitter(tempData$avgtemp[iffer2],3), col=rgb(0.2,0.2,0.2,0.1),pch=".")
}
pdf("stationstemp.pdf", width=9, height=6)
par(mfrow=c(2,3))
for(i in seq_along(stationNames)) plotTemps(i)
par(mfrow=c(1,1))
dev.off()
library(streamR)
library(lubridate)
library(stringr)
library(plyr)
install.packages('streamR')
install.packages("streamR")
install.packages('lubridate')
install.packages("lubridate")
library(streamR)
library(lubridate)
library(stringr)
library(plyr)
library(streamR)
library(lubridate)
library(stringr)
library(plyr)
filterStream("tweets_oscars.json", track = c("Oscars", "Oscars2014"),
timeout = 10800, oauth = twitCred)
tweets = parseTweets("tweets_oscars.json", simplify = TRUE)
Sys.setlocale("LC_TIME", "en_US.UTF-8")
dat$created_at[1]
dat$time = as.POSIXct(dat$created_at, tz = "UTC", format = "%a %b %d %H
:%M:%S %z %Y")
dat$round_hour = round_date(dat$time, unit = "hour")
plot_time = as.data.frame(table(dat$round_hour))
plot_time = plot_time[-nrow(plot_time),]
plot(plot_time[,2], type = "l", xaxt = "n", xlab = "Hour", ylab = "Frequency")
axis(1, at = c(1, 20, 40, 60), labels = plot_time[c(1, 20, 40, 60), 1])
unlist(dat[1234,])
actor = c(
"matthew mcconaughey",
"christian bale",
"bruce dern",
"leonardo dicaprio",
"chiwetel ejiofor"
)
actress = c(
"cate blanchett",
"amy adams",
"sandra bullock",
"judi dench",
"meryl streep"
)
film = c(
"(12|twelve) years a slave",
"american hustle",
"captain phillips",
"dallas buyers club",
"gravity",
"nebraska",
"philomena",
"(the )?wolf of wall street"
)
tmp_actor = lapply(dat$lotext, str_detect, actor)
dat_actor = ldply(tmp_actor)
colnames(dat_actor) = c("mcconaughey", "bale", "dern", "dicaprio", "ejiofor")
tmp_actress = lapply(dat$lotext, str_detect, actress)
dat_actress = ldply(tmp_actress)
colnames(dat_actress) = c("blanchett", "adams", "bullock", "dench", "streep")
tmp_film = lapply(dat$lotext, str_detect, film)
dat_film = ldply(tmp_film)
colnames(dat_film) = c("twelve_years", "american_hustle", "capt_phillips", "dallas_buyers", "gravity", "nebraska", "philomena", "wolf_wallstreet")
apply(dat_actor, 2, sum)
apply(dat_actress, 2, sum)
apply(dat_film, 2, sum)
tmp_actor2 = lapply(actor, agrep, dat$lotext)
length_actor = unlist(lapply(tmp_actor2, length))
names(length_actor) = c("mcconaughey", "bale", "dern", "dicaprio", "ejiofor")
tmp_actress2 = lapply(actress, agrep, dat$lotext)
length_actress = unlist(lapply(tmp_actress2, length))
names(length_actress) = c("blanchett", "adams", "bullock", "dench", "streep")
length_actor
length_actress
library(stringr)
library(RCurl)
library(XML)
library(maptools)
library(rgdal)
library(maps)
library(TeachingDemos)
tb <- getForm("http://www.dastelefonbuch.de/", .params = c(kw = "Feuerstein", cmd = "search", ao1 = "1", reccount = "2000"))
dir.create("phonebook_feuerstein")
write(tb, file =  "phonebook_feuerstein/phonebook_feuerstein.html")
tb_parse <- htmlParse("phonebook_feuerstein/phonebook_feuerstein.html",encoding="UTF-8")
xpath <- '//ul/li/a[contains(text(), "Privat")]'
num_results <- xpathSApply(tb_parse, xpath, xmlValue)
num_results
num_results <- as.numeric(str_extract(num_results, '[[:digit:]]+'))
num_results
xpath <- '//div[@class="name"]/a[@title]'
surnames[1:5]
surnames <- xpathSApply(tb_parse, xpath, xmlValue)
xpath <- '//span[@itemprop="postal-code"]'
zipcodes <- xpathSApply(tb_parse, xpath, xmlValue)
zipcodes[1:5]
length(surnames)
length(zipcodes)
names_vec <- xpathSApply(tb_parse, xpath, xmlValue)
xpath <- '//span[@itemprop="postal-code"]/ancestor::div[@class="popupMenu"]/preceding-sibling::div[@class="name"]'
names_vec[1:5]
names_vec <- str_replace_all(names_vec, "(\\n|\\t|\\r| {2,})", "")
xpath <- '//div[@class="name"]/following-sibling::div[@class="popupMenu"]//span[@itemprop="postal-code"]'
zipcodes_vec <- xpathSApply(tb_parse, xpath, xmlValue)
zipcodes_vec <- as.numeric(zipcodes_vec)
zipcodes_vec[1:5]
entries_df <- data.frame(plz = zipcodes_vec, name = names_vec)
head(entries_df)
dir.create("geo_germany")
download.file("http://fa-technik.adfc.de/code/opengeodb/PLZ.tab", destfile="geo_germany/plz_de.txt")
plz_df <- read.delim("geo_germany/plz_de.txt", stringsAsFactors=FALSE,encoding="UTF-8")
head(plz_df)
places_geo <- merge(entries_df, plz_df, by="plz", all.x = TRUE)
head(places_geo)
download.file("http://biogeo.ucdavis.edu/data/gadm2/shp/DEU_adm.zip", destfile="geo_germany/ger_shape.zip")
unzip("geo_germany/ger_shape.zip", exdir = "geo_germany")
dir("geo_germany")
projection <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
map_germany <- readShapePoly(paste(getwd(),"/geo_germany/DEU_adm0.shp", sep=""), proj4string=projection)
map_germany_laender <- readShapePoly(paste(getwd(),"/geo_germany/DEU_adm1.shp", sep=""), proj4string=projection)
coordinates <- SpatialPoints(cbind(places_geo$lon,places_geo$lat))
proj4string(coordinates) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
data("world.cities")
cities_ger <- world.cities[world.cities$country.etc == "Germany" & (world.cities$pop > 460000 | world.cities$name =="Mannheim"  | world.cities$name =="Jena"),]
dir.create("figures")
par(mfrow=c(1,1))
coords_cities <- SpatialPoints(cbind(cities_ger$long,cities_ger$lat))
plot(map_germany)
plot(map_germany_laender, add = T)
points(coordinates$coords.x1, coordinates$coords.x2, col=rgb(10,10,10,max=255), bg=rgb(10,10,10,max=255), pch=20, cex=1)
points(coords_cities, col = "black", , bg="grey", pch=23)
shadowtext(cities_ger$long,cities_ger$lat, labels = cities_ger$name, pos = 4, col = "black", bg = "white", cex =  1.2)
require(stringr)
require(XML)
require(RCurl)
if(!file.exists("dataExplore")) dir.create("dataExplore")
baseURL <- "http://www.amazon.com/s/ref=nb_sb_noss_2?url=node%3D2407749011&field-keywords="
keyword <- "Apple"
forceDownload <- F
readLine <- function(x) paste0(readLines(x, warn=F), collapse="\n")
fname <- paste0(keyword, " firstSearchPage.html")
if( !file.exists(fname) | forceDownload == T ){
url             <- paste0(baseURL, keyword)
firstSearchPage <- getURL(url)
writeLines(firstSearchPage, fname)
}else{
firstSearchPage <- readLine(fname)
}
parsedFirstSearchPage <- htmlParse(firstSearchPage)
xpath <- paste0('//span[@class="refinementLink" and text()="', keyword,'"]/../@href')
xpath
restrictedSearchPageLink <- xpathApply(parsedFirstSearchPage, xpath)
restrictedSearchPageLink <- unlist(as.character(restrictedSearchPageLink))
restrictedSearchPageLink <- paste0("http://www.amazon.com",
restrictedSearchPageLink)
restrictedSearchPageLink <- paste0(restrictedSearchPageLink,
"&sort=date-desc-rank")
restrictedSearchPageLink
fname <- paste0(keyword, " SearchPage 1.html")
if( !file.exists(fname) | forceDownload == T ){
restrictedSearchPage <- getURL(restrictedSearchPageLink)
writeLines(restrictedSearchPage, fname)
}else{
restrictedSearchPage <- readLine(fname)
}
xpath <- "//a[@class='pagnNext']/@href"
parsedRestrictedSearchPage <- htmlParse(restrictedSearchPage)
nextPageLink <- xpathApply(parsedRestrictedSearchPage, xpath)
SearchPages           <- list()
SearchPages[[1]]      <- restrictedSearchPage
xpath           <- "//a[@class='pagnNext']/@href"
for( i in 2:5 ){
fname <- paste0(keyword, " searchPage ",i,".html")
if( !file.exists(fname) | forceDownload == T ){
nextPageLink <- xpathApply( htmlParse(SearchPages[[ i-1 ]]), xpath)
nextPageLink <- paste0("http://www.amazon.com", nextPageLink)
SearchPages[[ i ]] <- getURL(nextPageLink)
writeLines(SearchPages[[ i ]], fname)
}else{
SearchPages[[ i ]] <- readLine(fname)
}
}
source("amazon_func.r")
if(!file.exists("dataFull") & grepl("dataFull",basename(getwd()))) dir.create("dataFull")
setwd("dataFull")
forceDownload = F
KeyWords = c("Apple", "BlackBerry", "HTC",
"LG", "Motorola", "Nokia", "Samsung")
n = 5
SearchPageList = NULL
SearchPagesKeywords = NULL
for(i in seq_along(KeyWords)){
message(KeyWords[i])
SearchPageList = c(SearchPageList,
getSearchPages(KeyWords[i], n, forceDownload)
)
SearchPagesKeywords = c( SearchPagesKeywords, rep(KeyWords[i], n) )
}
titles  = extractTitles(SearchPageList)
links   = extractLinks(SearchPageList)
if(!file.exists("phones.Rdata") | forceDownload==T){
brands = rep(KeyWords,each=n*24)
productPages = getProductPages(links, brands, forceDownload)
stars  = extractStars(productPages)
asins  = extractASINs(productPages)
models = extractModels(productPages)
ranks  = extractRanks(productPages)
prices = extractPrices(productPages)
fnames = paste0(brands, " ProductPage ", 1:(length(KeyWords)*n*24),".html" )
phones = data.frame(   brands, prices, stars, ranks, asins, models, titles, links, fnames,
timestamp=file.info(fnames)$ctime,
stringsAsFactors=F)
save(phones, stars, asins, models, ranks, prices, brands, timestamp, titles, fnames,
file="phones.Rdata")
}else{
load("phones.Rdata")
}
phonesClean = phones[complete.cases(phones),]
phonesClean = phonesClean[!duplicated(phonesClean$asins),]
mprices = rep(NA,length(phonesClean$prices))
for(i in seq_along(phonesClean$models)){
mprices[i] = mean(phonesClean$prices[phonesClean$models[i]==phonesClean$models], na.rm=T)
}
phonesClean$prices = round(mprices,2)
phonesClean = phonesClean[!duplicated(phonesClean$models),]
plotResults = function(X, title="") {
Prices = X$prices
Stars  = X$stars
Ranks  = X$ranks
plot(  Prices, Stars, pch=20, cex=10, col="white",
ylim=c(1,5),xlim=c(0,1000), main=title,
cex.main=2, cex.axis=2, xaxt="n")
axis(1, at=c(0,500,1000),labels=c(0,500,1000),cex.axis=2)
abline(v=seq(0,1000,100),col="grey")
abline(h=seq(0,5,1),col="grey")
points(Prices, Stars, col=rgb(0,0,0,0.2), pch=20, cex=7)
index = order(Ranks)[1:5]
abline(v=Prices[index],col="black")
abline(h=Stars[index],col="black")
points(Prices[index], Stars[index], col=rgb(1,1,1,1), pch=20, cex=2)
}
pdf("ranks.pdf", width=12, height=6)
par(mfrow=c(2,4))
par(mar=c(3,2,2,1))
par(oma=c(3, 4, 0, 1))
plotResults(phones, title="all")
for(i in seq_along(KeyWords)){
plotResults( phones[phones$brands==KeyWords[i], ], title=KeyWords[i] )
}
mtext("costumer ratings\n",2,outer=T,cex=1.5)
mtext("prices",1,outer=T,cex=1.5)
dev.off()
require(stringr)
require(XML)
require(RCurl)
require(RSQLite)
sqlite  = dbDriver("SQLite")
con     = dbConnect(sqlite, "amazonProductInfo.db")
sql = "SELECT phones.asin, items.fname FROM phones
JOIN items
ON phones.id=phones_id;"
phonesData = dbGetQuery(con, sql)
if(!file.exists("dataReviews")) dir.create("dataReviews")
setwd("dataReviews")
productPageFiles = paste0("../dataFull/", phonesData$fname)
productPages     = lapply(productPageFiles, htmlParse)
extractReviewLinks = function(x){
x = xpathApply(x, "//a[contains(text(), 'customer review')]/@href", as.character)[[1]]
if(length(x)==0) x = NA
if(str_detect(x,"create-review") & !is.na(x)) x = NA
names(x) = NULL
x
}
reviewLinks = unlist(lapply(productPages, extractReviewLinks))
for(i in seq_along(reviewLinks)){
if(is.na(reviewLinks[i])){
link = dirname(getwd())
link = paste0(link,str_replace(productPageFiles[i],"\\.\\.",""))
print(link)
}
}
reviewLinks = str_replace(reviewLinks,"http://www.amazon.com","")
reviewLinks = ifelse(is.na(reviewLinks),NA,paste0("http://www.amazon.com",reviewLinks))
for(i in seq_along(reviewLinks)){
fname = paste0(phonesData[i,"asin"], "_0001.html")
if(!file.exists(fname) & !is.na(reviewLinks[i])){
message("downloading")
try(download.file(reviewLinks[i],fname))
sleep = abs(rnorm(1)) + runif(1,0,0.25)
message(paste0("I have done ",i," of ",length(reviewLinks)," - gonna sleep ",round(sleep,2)," seconds."))
Sys.sleep(sleep)
}
message(paste0(i," size: ",file.info(fname)$size/1000)," KB")
}
firstPages = list.files(pattern="001.html")
file.remove(firstPages[file.info(firstPages)$size==0])
firstPages = list.files(pattern="001.html")
HTML = lapply(firstPages, htmlParse)
for(i in seq_along(HTML)){
link = xpathApply( HTML[[i]],
"//a[contains(text(), 'Next')]/@href",
as.character)[[1]]
k = 2
while(length(link) > 0 & k <= 5){
fname = str_replace(   firstPages[i], "[[:digit:]]{4}.html",
paste0(str_pad(k, 4, side = "left", pad = "0"),".html"))
message(paste0(i,":",k,"... :",fname))
if(!file.exists(fname) & length(link) > 0){
download.file(link, fname, quiet=T)
message(paste0(" download to file name: ",fname))
Sys.sleep(abs(rnorm(1)) + runif(1,0,0.25))
}
htmlNext = htmlParse(fname)
link = tryCatch(xpathApply( htmlNext,
"//a[contains(text(), 'Next')]/@href",
as.character)[[1]],
error = function(e){
message("xpath error")
NULL
})
k = k + 1
}
}
tmp = list.files(pattern=".html")
file.remove(tmp[file.info(tmp)$size < 50000])
getNumbers = function(node){
val = xmlValue(node)
x = str_extract(val,"[[:digit:]]{1,6}")
x
}
FPAsins = str_replace(firstPages,"_0001.html","")
reviewsMeta   = data.frame(asin=FPAsins,one=NA,two=NA,three=NA,four=NA,five=NA,stringsAsFactors=F)
for(i in seq_along(HTML)){
tmp = as.numeric(
readHTMLTable(  HTML[[i]],
elFun = getNumbers,
stringsAsFactors=F
)$productSummary$V3 )
print(tmp)
reviewsMeta[i,c("one","two","three","four","five")] = tmp[1:5]
}
reviewsMeta$sum = apply(reviewsMeta[,c("one","two","three","four","five")],1,sum)
reviewsMeta$mean = ( reviewsMeta$one + reviewsMeta$two*2 + reviewsMeta$three*3 +
reviewsMeta$four*4 + reviewsMeta$five*5) /    reviewsMeta$sum
reviews = data.frame(  asin=NA, stars=0, helpfulyes=0, helpfulno=0,
helpfulsum=0, date="", title="",text="", stringsAsFactors=F)
for(i in seq_along(FPAsins)){
files = list.files(pattern=FPAsins[i])
asin  = FPAsins[i]
message(paste0(i," / ",length(FPAsins)," ... doing: ",asin))
for(k in seq_along(files)){
html = htmlParse(files[k])
reviewValue  = unlist(xpathApply(html, "//div[@style='margin-left:0.5em;']",xmlValue))
helpful      = str_extract(reviewValue,"[[:digit:]]{1,5}.*?[[:digit:]]{1,5} people")
helpful      = str_extract_all(helpful,"[[:digit:]]{1,5}")
helpfulyes   = as.numeric(unlist(lapply(helpful,'[',1)))
helpfulno    = as.numeric(unlist(lapply(helpful,'[',2))) - helpfulyes
helpfulsum   = helpfulyes + helpfulno
stars        = str_extract(reviewValue,"[[:digit:]]\\.[[:digit:]] out of 5 stars")
stars        = as.numeric(str_extract(stars, "[[:digit:]]"))
text = unlist(xpathApply( html,
"//div[@style='margin-left:0.5em;']/div[@class='reviewText']",
xmlValue))
text = str_replace_all(text, "'","''")
title = xpathApply(
html,
"//div[@style='margin-left:0.5em;']/div/span[@style='vertical-align:middle;']/b",
xmlValue)
title  = unlist(title)
title = str_replace_all(title, "'","''")
date = xpathApply(
html,
"//div[@style='margin-left:0.5em;']/div/span[@style='vertical-align:middle;']/nobr",
xmlValue)
}
date = unlist(date)
tmp = cbind(asin,stars,helpfulyes,helpfulno,helpfulsum,date,title,text)
reviews = rbind(reviews,tmp)
}
reviews = reviews[!is.na(reviews$asin),]
SQL = paste0(" INSERT INTO reviewsMeta
(asin, one, two, three, four, five)
VALUES
('",
paste(  reviewsMeta[,"asin"], reviewsMeta[,"one"],
reviewsMeta[,"two"],  reviewsMeta[,"three"],
reviewsMeta[,"four"], reviewsMeta[,"five"],
sep="', '")
,"'); ")
for(i in seq_along(SQL)) dbGetQuery(con,SQL[i])
SQL = paste0(" INSERT INTO reviews
( asin, stars, helpfulyes, helpfulno,
helpfulsum, date, title, text)
VALUES
('",
paste(  reviews[,"asin"], reviews[,"stars"],
reviews[,"helpfulyes"],  reviews[,"helpfulno"],
reviews[,"helpfulsum"], reviews[,"date"],
reviews[,"title"], reviews[,"text"],
sep="', '")
,"'); ")
for(i in seq_along(SQL)){
dbGetQuery(con,SQL[i])
}
